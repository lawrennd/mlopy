#!/usr/bin/env python
from netlab import *
import numpy as np
import matplotlib.pyplot as pp
from matplotlib.font_manager import fontManager, FontProperties

#"""DEMOLGD1 Demonstrate simple MLP optimisation with on-line gradient descent
#
#	Description
#	The problem consists of one input variable X and one target variable
#	T with data generated by sampling X at equal intervals and then
#	generating target data by computing SIN(2*PI*X) and adding Gaussian
#	noise. A 2-layer network with linear outputs is trained by minimizing
#	a  sum-of-squares error function using on-line gradient descent.
#
#	See also
#	DEMMLP1, OLGD
#

#	Copyright (c) Ian T Nabney (1996-2001)


# Generate the matrix of inputs x and targets t.

ndata = 20			# Number of data points.
noise = 0.2			# Standard deviation of noise distribution.
x = np.linspace(0.0, 1.0, ndata).reshape((-1, 1))
np.random.seed(42)
t = np.sin(2*np.pi*x) + noise*np.random.randn(ndata, 1)

clc()
print "This demonstration illustrates the use of the on-line gradient"
print "descent algorithm to train a Multi-Layer Perceptron network for"
print "regression problems.  It is intended to illustrate the drawbacks"
print "of this algorithm compared to more powerful non-linear optimisation"
print "algorithms, such as conjugate gradients."
print " "
print "First we generate the data from a noisy sine function and construct"
print "the network."
print " "
raw_input('Press return to continue.')

# Set up network parameters.
nin = 1			# Number of inputs.
nhidden = 3			# Number of hidden units.
nout = 1			# Number of outputs.
alpha = 0.01			# Coefficient of weight-decay prior. 

# Create and initialize network weight vector.
net = mlp(nin, nhidden, nout, 'linear')
# Initialise weights reasonably close to 0
net.init(10.0)

# Set up vector of options for the optimiser.
options = foptions()
options[0] = 1			# This provides display of error values.
options[13] = 20		# Number of training cycles. 
options[17] = 0.1		# Learning rate
options[16] = 0.4		# Momentum
options[4] = 1 		# Do randomise pattern order
clc()
print "Then we set the options for the training algorithm."
print "In the first phase of training, which lasts for ", str(options[13]), " cycles,"
print "the learning rate is ", str(options[17]), " and the momentum is ", str(options[16]), "."
print "The error values are displayed at the end of each pass through the"
print "entire pattern set."
print " "
raw_input('Press return to continue.')


# Train using online gradient descent
olgd(net, options, x, t)

# Now allow learning rate to decay and remove momentum
options[1] = 0
options[2] = 0
options[16] = 0.4	# Turn off momentum
options[4] = 1		# Randomise pattern order
options[5] = 1		# Set learning rate decay on
options[13] = 200
options[17] = 0.1	# Initial learning rate

print "In the second phase of training, which lasts for up to ", str(options[13]), " cycles,"
print "the learning rate starts at ", str(options[17]), ", decaying at 1/t and the momentum is ", str(options[16]), "."
print " "
raw_input('Press return to continue.')

olgd(net, options, x, t);
clc()
print "Now we plot the data, underlying function, and network outputs"
print "on a single graph to compare the results."
print " "
raw_input('Press return to continue.')


# Plot the data, the original function, and the trained network function.
plotvals = np.linspace(0, 1, 100).reshape(-1, 1)
y = net.fwd(plotvals)[0]
fh1 = pp.figure()
pp.plot(x, t, 'ob', label='data')
pp.hold(True)
pp.axis([0, 1, -1.5, 1.5])
fy = np.sin(2*np.pi*plotvals)
pp.plot(plotvals, fy, '-g', linewidth=2, label='function')
pp.plot(plotvals, y, '-r', label='network')

pp.hold(False)

print "Note the very poor fit to the data: this should be compared with"
print "the results obtained in demmlp1."
print " "
raw_input('Press return to exit.')

pp.close(fh1)
