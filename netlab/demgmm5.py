#!/usr/bin/env python
from netlab import *
import numpy as np
import matplotlib.pyplot as pp

#"""DEMGMM5 Demonstrate density modelling with a PPCA mixture model.
#
#	Description
#	 The problem consists of modelling data generated by a mixture of
#	three Gaussians in 2 dimensions with a mixture model using full
#	covariance matrices.  The priors are 0.3, 0.5 and 0.2; the centres
#	are (2, 3.5), (0, 0) and (0,2); the variances are (0.16, 0.64) axis
#	aligned, (0.25, 1) rotated by 30 degrees and the identity matrix. The
#	first figure contains a scatter plot of the data.
#
#	A mixture model with three one-dimensional PPCA components is trained
#	using EM.  The parameter vector is printed before training and after
#	training.  The parameter vector consists of priors (the column), and
#	centres (given as (x, y) pairs as the next two columns).
#
#	The second figure is a 3 dimensional view of the density function,
#	while the third shows the axes of the 1-standard deviation ellipses
#	for the three components of the mixture model together with the one
#	standard deviation along the principal component of each mixture
#	model component.
#
#	See also
#	GMM, GMMINIT, GMMEM, GMMPROB, PPCA
#

#	Copyright (c) Ian T Nabney (1996-2001)
#       and Neil D. Lawrence (2009) (translation to python)"""


ndata = 500
data = np.random.randn(ndata, 2)
prior = np.array([0.3, 0.5, 0.2])
# Mixture model swaps clusters 1 and 3
datap = np.array([0.2, 0.5, 0.3])
datac = np.array([[0, 2], [ 0, 0], [ 2, 3.5]])
datacov = np.zeros((2, 2, 3))
for i in range(3):
    datacov[:, :, i] = np.eye(2)
data1 = data[0:round(prior[0]*ndata),:]
data2 = data[round(prior[0]*ndata):round((prior[1]+prior[0])*ndata), :]
data3 = data[round((prior[0]+prior[1])*ndata):ndata, :]

# First cluster has axis aligned variance and centre (2, 3.5)
data1[:, 0] = data1[:, 0]*0.1 + 2.0
data1[:, 1] = data1[:, 1]*0.8 + 3.5
datacov[:, :, 2] = np.array([[0.1*0.1, 0], [ 0, 0.8*0.8]])

# Second cluster has variance axes rotated by 30 degrees and centre (0, 0)
rotn = np.array([[np.cos(np.pi/6), -np.sin(np.pi/6)], [ np.sin(np.pi/6), np.cos(np.pi/6)]])
data2[:,0] = data2[:, 0]*0.2
data2 = np.dot(data2,rotn)
datacov[:, :, 1] = np.dot(np.dot(rotn.T,np.array([[0.04, 0], [ 0, 1]])), rotn)

# Third cluster is at (0,2)
data3[:, 1] = data3[:, 1]*0.1
data3 = data3 + np.tile(np.array([0, 2]), (round(prior[2]*ndata), 1))

# Put the dataset together again
data = np.r_[data1, data2, data3]

# ndata = 100			# Number of data points.
# noise = 0.2			# Standard deviation of noise distribution.
# x = np.r_[0.0:0.5:1.0/round(2*(ndata - 1))]
# np.random.seed(1)
# t = np.sin(2*np.pi*x) + noise*np.random.randn(ndata, 1)

# Fit three one-dimensional PPCA models
ncentres = 3
ppca_dim = 1

clc()
print "This demonstration illustrates the use of a Gaussian mixture model"
print "with a probabilistic PCA covariance structure to approximate the"
print "unconditional probability density of data in a two-dimensional space."
print "We begin by generating the data from a mixture of three Gaussians and"
print "plotting it."
print " "
print "The first cluster has axis aligned variance and centre (0, 2)."
print "The variance parallel to the x-axis is significantly greater"
print "than that parallel to the y-axis."
print "The second cluster has variance axes rotated by 30 degrees"
print "and centre (0, 0).  The third cluster has significant variance"
print "parallel to the y-axis and centre (2, 3.5)."
print " "
raw_input('Press return to continue.')

fh1 = pp.figure()
pp.axes(axisbg='w', frameon=True)
pp.plot(data[:, 0], data[:, 1], 'o')
pp.axis('equal')
pp.hold(True)

mix = gmm(2, ncentres, 'ppca', ppca_dim)
options = foptions()
options[13] = 10
options[0] = -1  # Switch off all warnings

# Just use 10 iterations of k-means in initialisation
# Initialise the model parameters from the data
mix.init(data, options)
print "The mixture model has three components with 1-dimensional"
print "PPCA subspaces.  The model parameters after initialisation using"
print "the k-means algorithm are as follows"
print "    Priors        Centres"
print np.c_[mix.priors, mix.centres]
print " "
pp.show()
raw_input('Press return to continue')
pp.ioff()

options[0]  = 1		# Prints out error values.
options[13] = 30		# Number of iterations.

print "We now train the model using the EM algorithm for up to 30 iterations."
print " "
raw_input('Press return to continue.')

errlog = mix.em(data, options, returnFlog=True)
print "The trained model has priors and centres:"
print "    Priors        Centres"
print np.c_[mix.priors, mix.centres]

# Now plot the result
for i in range(mix.ncentres):
    # Plot the PC vectors
    v = mix.U[:,:,i]
    start=mix.centres[i,:]-np.sqrt(mix.lambd[i])*v.flatten()
    endpt=mix.centres[i,:]+np.sqrt(mix.lambd[i])*v.flatten()
    linex = np.array([start[0], endpt[0]])
    liney = np.array([start[1], endpt[1]])
    pp.plot(linex, liney, color='k', linewidth=3)
    # Plot ellipses of one standard deviation
    theta = np.r_[0:2*np.pi:0.02]
    x = np.sqrt(mix.lambd[i])*np.cos(theta)
    y = np.sqrt(mix.covars[i])*np.sin(theta)
    # Rotate ellipse axes
    rot_matrix = np.array([[v[0, 0], -v[1,0]], [v[1,0], v[0,0]]])
    ellipse = np.dot(rot_matrix, np.c_[x, y].T).T
    # Adjust centre
    ellipse = ellipse + mix.centres[i,:]
    pp.plot(ellipse[:,0], ellipse[:,1], 'r-')

print " "
pp.show()
raw_input('Press return to exit')
pp.close(fh1)
