#!/usr/bin/env python
from netlab import *
import numpy as np
import matplotlib.pyplot as pp
from matplotlib.font_manager import fontManager, FontProperties

#"""DEMMLP1 Demonstrate simple regression using a multi-layer perceptron
#
#	Description
#	The problem consists of one input variable X and one target variable
#	T with data generated by sampling X at equal intervals and then
#	generating target data by computing SIN(2*PI*X) and adding Gaussian
#	noise. A 2-layer network with linear outputs is trained by minimizing
#	a  sum-of-squares error function using the scaled conjugate gradient
#	optimizer.
#
#	See also
#	MLP, MLPERR, MLPGRAD, SCG
#

#	Copyright (c) Ian T Nabney (1996-2001)
#        and Neil D. Lawrence (2009) (translation to python)"""

# Generate the matrix of inputs x and targets t.


ndata = 20			# Number of data points.
noise = 0.2			# Standard deviation of noise distribution.
x = np.linspace(0.0, 1.0, ndata).reshape((-1, 1))
np.random.seed(1)
t = np.sin(2*np.pi*x) + noise*np.random.randn(ndata, 1)

clc()
print "This demonstration illustrates the use of a Multi-Layer Perceptron"
print "network for regression problems.  The data is generated from a noisy"
print "sine function."
print " "
raw_input("Press return to continue.")

# Set up network parameters.
nin = 1			# Number of inputs.
nhidden = 3			# Number of hidden units.
nout = 1			# Number of outputs.
alpha = 0.01			# Coefficient of weight-decay prior. 

# Create and initialize network weight vector.

net = mlp(nin, nhidden, nout, 'linear', alpha)

# Set up vector of options for the optimiser.

options = np.zeros(18)
options[0] = 1			# This provides display of error values.
options[13] = 100		# Number of training cycles. 

clc()
print "The network has ", nhidden, " hidden units and a weight decay"
print "coefficient of ", str(alpha), "."
print " "
print "After initializing the network, we train it use the scaled conjugate"
print "gradients algorithm for 100 cycles."
print " "
raw_input("Press return to continue")

# Train using scaled conjugate gradients.
net = netopt(net, options, x, t, scg)

print " "
print "Now we plot the data, underlying function, and network outputs"
print "on a single graph to compare the results."
print " "
raw_input("Press return to continue.")

# Plot the data, the original function, and the trained network function.
plotvals = np.c_[0:1.01:0.01]
y = net.fwd(plotvals)[0]
fh1 = pp.figure()
pp.plot(x, t, 'ob', label='data')
pp.hold(True)
pp.xlabel('Input')
pp.ylabel('Target')
pp.axis([0, 1, -1.5, 1.5])
fy = np.sin(2*np.pi*plotvals);
pp.plot(plotvals, fy, '-k', linewidth=2, label='function')
pp.plot(plotvals, y, '-', linewidth=2, label='network')
font=FontProperties(size='small');
pp.legend(numpoints=1,prop=font)
pp.show()
raw_input("Press return to end.")
pp.close(fh1)

